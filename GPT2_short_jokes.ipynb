{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2-short-jokes",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataSci-rigo/hum0rB0T3000/blob/master/GPT2_short_jokes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK_lPvRlBdMB",
        "colab_type": "text"
      },
      "source": [
        "# Finetune GPT-2\n",
        "This colab demonstrates how to fine-tune GPT-2 on a dataset of presidential speeches. We use the [Hugging Face Transformer](https://github.com/huggingface/transformers) library in order to do this.\n",
        "\n",
        "**IMPORTANT: Make sure that you have GPU set as your Hardware Accelerator in `Runtime > Change runtime type` before running this Colab.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1cTndTh9HZN",
        "colab_type": "code",
        "outputId": "8ee3d7ff-3286-42f2-a719-7925a71ece07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -r ./apex/requirements.txt\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW7a-LM0-CuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!sh setup.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biV1z0koaDHT",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGscdaCtpmbV",
        "colab_type": "text"
      },
      "source": [
        "### Install HuggingFace Transfomers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicio9FLPv5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "\n",
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "!pip install dict_to_obj\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weh0BoPfk1zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import run_language_modeling\n",
        "import run_generation\n",
        "from dict_to_obj import DictToObj\n",
        "import collections\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ6FFHiMMP0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "import boto3\n",
        "\n",
        "s3r = boto3.resource('s3', aws_access_key_id='AKIA32S7UTDXTR5GRXX6',\n",
        "    aws_secret_access_key='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')\n",
        "buck = s3r.Bucket('humorbot3000')\n",
        "buck.download_file('1300_1n.txt','/content/1300_1n.txt')\n",
        "buck.download_file('rewritten_jokes.txt','/content/rewritten_jokes.txt')\n",
        "buck.download_file('shuffle_rewritten.txt','/content/shuffle_rewritten.txt')\n",
        "buck.download_file('shuffle_rewritten_cls.txt','/content/shuffle_rewritten_cls.txt')\n",
        "buck.download_file('shuffle_puns.txt','/content/shuffle_puns.txt')\n",
        "buck.download_file('shuffle_puns_cls.txt','/content/shuffle_puns_cls.txt')\n",
        "buck.download_file('shuffle_puns_label.txt','/content/shuffle_puns_label.txt')\n",
        "buck.download_file('shuffle_short_cls.txt','/content/shuffle_short_cls.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VzV8iTrphJl",
        "colab_type": "text"
      },
      "source": [
        "## Finetune and Eval\n",
        "The Hugging Face library provides a script [run_language_modeling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py) which contains all of the code for training and evaluating a language model.\n",
        "\n",
        "We will be calling this script directly from the command line in order to launch training. We will also use functions from this script to conduct evaluation and generate samples at inference time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzFhwDSqOg3",
        "colab_type": "text"
      },
      "source": [
        "### Launch fine-tuninng\n",
        "We will be calling `run_language_modeling.py` from the command line to launch fine-tuning, **Running fine-tuning may take several hours.** Every `save_steps` steps, a checkpoint is saved to disk. The checkpoint contains all the learned weights for your model, and you can  always reload the model from a saved checkpoint, even if your Colab has crashed.\n",
        "\n",
        "Below is an explanation of some of the arguments you might want to modify in the command below. \n",
        "\n",
        "* `--line_by_line`: Add `--line_by_line` if distinct lines of the text should be treated as distinct training examples. For example, if your dataset contains one story/tweet/article per line, this should be set.\n",
        "* `--num_train_epochs`: The number of times to iterate over the train set. Increasing the number of epochs may result in better performance, but making this number too high will cause the model to overfit on the train set.\n",
        "* `--block_size`: Your training text is truncated into blocks of this length. At test time, you will only want to generate sequences that are at most this length.\n",
        "* `--gradient_accumulation_steps`: Update the model weights every this many steps. You shold set this to >1 when the batch size is very small to improve training stability.\n",
        "* `--output_dir`: This is the where checkpoints will get saved. When you finetune on your own dataset, you should change this path. We recommend saving checkpoints to your Google Drive (`/content/drive/My Drive/`) so you can access them even if the Colab session dies.\n",
        "* `--model_name_or_path` The path to the model weights to use when starting fine-tuning. You can set this to `gpt2-medium` to initialize with GPT-2's 355 million parameter model, or `gpt2` to initialize with their smaller 124 million parameter model. You can also set this to one of your own checkpoints to restart your training job if it crashes.\n",
        "\n",
        "**I am getting out-of memory errors. What do I do?**\n",
        "\n",
        "The number of trainable paramters in the model is a function of the `block_size` and the `batch_size`. If you are getting out-of-memory errors, then try drecreasing these value.\n",
        "\n",
        "**Oh no! My computer went to sleep and the Colab disconnected.**\n",
        "\n",
        "The train job might have still completed. Check the `output_dir` in your Google Drive to see if checkpoint files have been created there.\n",
        "\n",
        "**Training is taking foreverrrrrr.**\n",
        "\n",
        "Try decreasing `num_train_epochs` or changing `model_name_or_path` to `gpt2` instead of `gpt2-medium`.\n",
        "If your evaluation set is very large, you might also want to remove the `evaluate_during_training` flag or increase `logging_steps`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33GutF1QVEV",
        "colab_type": "code",
        "outputId": "3ab3e0d5-91fe-435a-8010-6f25ca9c06ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!mkdir /content/checkpoint-1500\n",
        "!mkdir /content/output\n",
        "train_ON ='/content/shuffle_short_cls.txt'\n",
        "!python run_language_modeling.py \\\n",
        "    --output_dir='/content/short_jokes' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2-large \\\n",
        "    --save_total_limit=1 \\\n",
        "    --num_train_epochs=1.0 \\\n",
        "    --do_train \\\n",
        "    --logging_steps=10000 \\\n",
        "    --save_steps=10000 \\\n",
        "    --train_data_file={train_ON} \\\n",
        "    --per_gpu_train_batch_size=1 \\\n",
        "    --block_size=60 \\\n",
        "    --gradient_accumulation_steps=128\\\n",
        "    --overwrite_output_dir\\\n",
        "    --line_by_line\n",
        "\n",
        "import os\n",
        "\n",
        "def upload_objects(my_bucket, directory_name ):\n",
        "    try:\n",
        "      root_path='/content'\n",
        "\n",
        "      for path, subdirs, files in os.walk(root_path):\n",
        "          path = path.replace(\"\\\\\",\"/\")\n",
        "          directory_name = path.replace(root_path,\"\")\n",
        "          for file in files:\n",
        "              my_bucket.upload_file(os.path.join(path, file), directory_name+'/'+file)\n",
        "\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "upload_objects(buck,'/content/short_jokes')\n",
        "'''!python run_language_modeling.py \\\n",
        "    --output_dir='/content/output' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2-medium \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=2.0 \\\n",
        "    --do_train \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps=500 \\\n",
        "    --save_steps=500 \\\n",
        "    --train_data_file=/content/presidential_speeches_train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=/content/presidential_speeches_valid.txt \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=128 \\\n",
        "    --gradient_accumulation_steps=5'''\n",
        "#pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/checkpoint-1500’: File exists\n",
            "mkdir: cannot create directory ‘/content/output’: File exists\n",
            "04/14/2020 22:08:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/14/2020 22:08:08 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-config.json from cache at /root/.cache/torch/transformers/c8f887cdfff4327916f4b7ed06a379c0add42bd9c66e1fe3b4a5a8525a4b2678.e4da68877e47676a9ceb9fb82e7c751246f61bc204d806ebd36e86911e825095\n",
            "04/14/2020 22:08:08 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1280,\n",
            "  \"n_head\": 20,\n",
            "  \"n_layer\": 36,\n",
            "  \"n_positions\": 1024,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257,\n",
            "  \"xla_device\": null\n",
            "}\n",
            "\n",
            "04/14/2020 22:08:09 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-config.json from cache at /root/.cache/torch/transformers/c8f887cdfff4327916f4b7ed06a379c0add42bd9c66e1fe3b4a5a8525a4b2678.e4da68877e47676a9ceb9fb82e7c751246f61bc204d806ebd36e86911e825095\n",
            "04/14/2020 22:08:09 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1280,\n",
            "  \"n_head\": 20,\n",
            "  \"n_layer\": 36,\n",
            "  \"n_positions\": 1024,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257,\n",
            "  \"xla_device\": null\n",
            "}\n",
            "\n",
            "04/14/2020 22:08:09 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-vocab.json from cache at /root/.cache/torch/transformers/69f8d734111f39eaa51a85907bfdc81a7ef42242d638ffab6f77df305402b2b2.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "04/14/2020 22:08:09 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-merges.txt from cache at /root/.cache/torch/transformers/38d28acc17953e356348dca948e152c653c0ccf5058a552eea30168e27f02046.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "04/14/2020 22:08:09 - INFO - filelock -   Lock 140163640065720 acquired on /root/.cache/torch/transformers/bcc61dff8b1b03d0fd33a1eb1dc4db00875cae33296848155c6882d4bab03db4.999a50942f8e31ea6fa89ec2580cb38fa40e3db5aa46102d0406bcfa77d9142d.lock\n",
            "04/14/2020 22:08:09 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp_kzplpfq\n",
            "Downloading: 100% 3.25G/3.25G [01:02<00:00, 52.1MB/s]\n",
            "04/14/2020 22:09:12 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin in cache at /root/.cache/torch/transformers/bcc61dff8b1b03d0fd33a1eb1dc4db00875cae33296848155c6882d4bab03db4.999a50942f8e31ea6fa89ec2580cb38fa40e3db5aa46102d0406bcfa77d9142d\n",
            "04/14/2020 22:09:12 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/bcc61dff8b1b03d0fd33a1eb1dc4db00875cae33296848155c6882d4bab03db4.999a50942f8e31ea6fa89ec2580cb38fa40e3db5aa46102d0406bcfa77d9142d\n",
            "04/14/2020 22:09:12 - INFO - filelock -   Lock 140163640065720 released on /root/.cache/torch/transformers/bcc61dff8b1b03d0fd33a1eb1dc4db00875cae33296848155c6882d4bab03db4.999a50942f8e31ea6fa89ec2580cb38fa40e3db5aa46102d0406bcfa77d9142d.lock\n",
            "04/14/2020 22:09:12 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin from cache at /root/.cache/torch/transformers/bcc61dff8b1b03d0fd33a1eb1dc4db00875cae33296848155c6882d4bab03db4.999a50942f8e31ea6fa89ec2580cb38fa40e3db5aa46102d0406bcfa77d9142d\n",
            "04/14/2020 22:09:34 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias', 'h.24.attn.masked_bias', 'h.25.attn.masked_bias', 'h.26.attn.masked_bias', 'h.27.attn.masked_bias', 'h.28.attn.masked_bias', 'h.29.attn.masked_bias', 'h.30.attn.masked_bias', 'h.31.attn.masked_bias', 'h.32.attn.masked_bias', 'h.33.attn.masked_bias', 'h.34.attn.masked_bias', 'h.35.attn.masked_bias', 'lm_head.weight']\n",
            "04/14/2020 22:09:50 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=60, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file=None, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=128, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=10000, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2-large', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/short_jokes', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=1, save_steps=10000, save_total_limit=1, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/shuffle_short_cls.txt', warmup_steps=0, weight_decay=0.0)\n",
            "04/14/2020 22:09:50 - INFO - __main__ -   Creating features from dataset file at /content/shuffle_short_cls.txt\n",
            "04/14/2020 22:10:34 - INFO - __main__ -   ***** Running training *****\n",
            "04/14/2020 22:10:34 - INFO - __main__ -     Num examples = 231657\n",
            "04/14/2020 22:10:34 - INFO - __main__ -     Num Epochs = 1\n",
            "04/14/2020 22:10:34 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "04/14/2020 22:10:34 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "04/14/2020 22:10:34 - INFO - __main__ -     Gradient Accumulation steps = 128\n",
            "04/14/2020 22:10:34 - INFO - __main__ -     Total optimization steps = 1809\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/231657 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/231657 [00:01<77:46:45,  1.21s/it]\u001b[A\n",
            "Iteration:   0% 2/231657 [00:01<56:43:40,  1.13it/s]\u001b[A\n",
            "Iteration:   0% 3/231657 [00:01<41:42:53,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 4/231657 [00:01<31:24:54,  2.05it/s]\u001b[A\n",
            "Iteration:   0% 5/231657 [00:01<24:02:12,  2.68it/s]\u001b[A\n",
            "Iteration:   0% 6/231657 [00:01<18:57:36,  3.39it/s]\u001b[A\n",
            "Iteration:   0% 7/231657 [00:01<16:20:35,  3.94it/s]\u001b[A\n",
            "Iteration:   0% 8/231657 [00:02<13:40:15,  4.71it/s]\u001b[A\n",
            "Iteration:   0% 9/231657 [00:02<11:51:59,  5.42it/s]\u001b[A\n",
            "Iteration:   0% 10/231657 [00:02<10:34:23,  6.09it/s]\u001b[A\n",
            "Iteration:   0% 11/231657 [00:02<9:38:14,  6.68it/s] \u001b[A\n",
            "Iteration:   0% 12/231657 [00:02<9:07:54,  7.05it/s]\u001b[A\n",
            "Iteration:   0% 13/231657 [00:02<8:39:31,  7.43it/s]\u001b[A\n",
            "Iteration:   0% 14/231657 [00:02<8:19:43,  7.73it/s]\u001b[A\n",
            "Iteration:   0% 15/231657 [00:02<8:02:35,  8.00it/s]\u001b[A\n",
            "Iteration:   0% 16/231657 [00:02<7:55:06,  8.13it/s]\u001b[A\n",
            "Iteration:   0% 17/231657 [00:03<7:59:00,  8.06it/s]\u001b[A\n",
            "Iteration:   0% 18/231657 [00:03<7:52:46,  8.17it/s]\u001b[A\n",
            "Iteration:   0% 19/231657 [00:03<7:43:50,  8.32it/s]\u001b[A\n",
            "Iteration:   0% 20/231657 [00:03<7:29:49,  8.58it/s]\u001b[A\n",
            "Iteration:   0% 21/231657 [00:03<7:17:44,  8.82it/s]\u001b[A\n",
            "Iteration:   0% 22/231657 [00:03<7:09:28,  8.99it/s]\u001b[A\n",
            "Iteration:   0% 23/231657 [00:03<7:19:21,  8.79it/s]\u001b[A\n",
            "Iteration:   0% 24/231657 [00:03<7:31:00,  8.56it/s]\u001b[A\n",
            "Iteration:   0% 25/231657 [00:04<7:42:20,  8.35it/s]\u001b[A\n",
            "Iteration:   0% 26/231657 [00:04<7:34:47,  8.49it/s]\u001b[A\n",
            "Iteration:   0% 27/231657 [00:04<7:30:26,  8.57it/s]\u001b[A\n",
            "Iteration:   0% 28/231657 [00:04<7:18:31,  8.80it/s]\u001b[A\n",
            "Iteration:   0% 29/231657 [00:04<7:15:42,  8.86it/s]\u001b[A\n",
            "Iteration:   0% 30/231657 [00:04<7:19:50,  8.78it/s]\u001b[A\n",
            "Iteration:   0% 31/231657 [00:04<7:11:41,  8.94it/s]\u001b[A\n",
            "Iteration:   0% 32/231657 [00:04<7:00:46,  9.17it/s]\u001b[A\n",
            "Iteration:   0% 33/231657 [00:04<6:55:57,  9.28it/s]\u001b[A\n",
            "Iteration:   0% 34/231657 [00:05<7:10:47,  8.96it/s]\u001b[A\n",
            "Iteration:   0% 35/231657 [00:05<7:39:04,  8.41it/s]\u001b[A\n",
            "Iteration:   0% 36/231657 [00:05<7:26:59,  8.64it/s]\u001b[A\n",
            "Iteration:   0% 37/231657 [00:05<7:20:56,  8.75it/s]\u001b[A\n",
            "Iteration:   0% 38/231657 [00:05<7:29:58,  8.58it/s]\u001b[A\n",
            "Iteration:   0% 39/231657 [00:05<7:40:17,  8.39it/s]\u001b[A\n",
            "Iteration:   0% 40/231657 [00:05<7:34:37,  8.49it/s]\u001b[A\n",
            "Iteration:   0% 41/231657 [00:05<7:25:07,  8.67it/s]\u001b[A\n",
            "Iteration:   0% 42/231657 [00:05<7:45:59,  8.28it/s]\u001b[A\n",
            "Iteration:   0% 43/231657 [00:06<7:42:29,  8.35it/s]\u001b[A\n",
            "Iteration:   0% 44/231657 [00:06<7:46:52,  8.27it/s]\u001b[A\n",
            "Iteration:   0% 45/231657 [00:06<7:48:12,  8.24it/s]\u001b[A\n",
            "Iteration:   0% 46/231657 [00:06<7:49:30,  8.22it/s]\u001b[A\n",
            "Iteration:   0% 47/231657 [00:06<8:00:46,  8.03it/s]\u001b[A\n",
            "Iteration:   0% 48/231657 [00:06<7:55:11,  8.12it/s]\u001b[A\n",
            "Iteration:   0% 49/231657 [00:06<8:05:32,  7.95it/s]\u001b[A\n",
            "Iteration:   0% 50/231657 [00:06<8:01:29,  8.02it/s]\u001b[A\n",
            "Iteration:   0% 51/231657 [00:07<7:49:12,  8.23it/s]\u001b[A\n",
            "Iteration:   0% 52/231657 [00:07<7:34:11,  8.50it/s]\u001b[A\n",
            "Iteration:   0% 53/231657 [00:07<7:22:35,  8.72it/s]\u001b[A\n",
            "Iteration:   0% 54/231657 [00:07<7:25:33,  8.66it/s]\u001b[A\n",
            "Iteration:   0% 55/231657 [00:07<7:27:23,  8.63it/s]\u001b[A\n",
            "Iteration:   0% 56/231657 [00:07<7:23:26,  8.70it/s]\u001b[A\n",
            "Iteration:   0% 57/231657 [00:07<7:23:34,  8.70it/s]\u001b[A\n",
            "Iteration:   0% 58/231657 [00:07<7:23:12,  8.71it/s]\u001b[A\n",
            "Iteration:   0% 59/231657 [00:08<7:26:53,  8.64it/s]\u001b[A\n",
            "Iteration:   0% 60/231657 [00:08<7:26:27,  8.65it/s]\u001b[A\n",
            "Iteration:   0% 61/231657 [00:08<7:26:41,  8.64it/s]\u001b[A\n",
            "Iteration:   0% 62/231657 [00:08<7:29:16,  8.59it/s]\u001b[A\n",
            "Iteration:   0% 63/231657 [00:08<7:38:54,  8.41it/s]\u001b[A\n",
            "Iteration:   0% 64/231657 [00:08<7:24:42,  8.68it/s]\u001b[A\n",
            "Iteration:   0% 65/231657 [00:08<7:13:43,  8.90it/s]\u001b[A\n",
            "Iteration:   0% 66/231657 [00:08<7:08:56,  9.00it/s]\u001b[A\n",
            "Iteration:   0% 67/231657 [00:08<7:18:22,  8.81it/s]\u001b[A\n",
            "Iteration:   0% 68/231657 [00:09<7:07:40,  9.03it/s]\u001b[A\n",
            "Iteration:   0% 69/231657 [00:09<7:08:12,  9.01it/s]\u001b[A\n",
            "Iteration:   0% 70/231657 [00:09<7:12:18,  8.93it/s]\u001b[A\n",
            "Iteration:   0% 71/231657 [00:09<7:11:23,  8.95it/s]\u001b[A\n",
            "Iteration:   0% 72/231657 [00:09<7:11:05,  8.95it/s]\u001b[A\n",
            "Iteration:   0% 73/231657 [00:09<7:42:16,  8.35it/s]\u001b[A\n",
            "Iteration:   0% 74/231657 [00:09<7:41:07,  8.37it/s]\u001b[A\n",
            "Iteration:   0% 75/231657 [00:09<7:40:54,  8.37it/s]\u001b[A\n",
            "Iteration:   0% 76/231657 [00:09<7:50:56,  8.20it/s]\u001b[A\n",
            "Iteration:   0% 77/231657 [00:10<7:57:10,  8.09it/s]\u001b[A\n",
            "Iteration:   0% 78/231657 [00:10<7:32:10,  8.54it/s]\u001b[A\n",
            "Iteration:   0% 79/231657 [00:10<7:17:30,  8.82it/s]\u001b[A\n",
            "Iteration:   0% 80/231657 [00:10<7:04:18,  9.10it/s]\u001b[A\n",
            "Iteration:   0% 81/231657 [00:10<6:59:08,  9.21it/s]\u001b[A\n",
            "Iteration:   0% 82/231657 [00:10<7:12:21,  8.93it/s]\u001b[A\n",
            "Iteration:   0% 83/231657 [00:10<7:25:04,  8.67it/s]\u001b[A\n",
            "Iteration:   0% 84/231657 [00:10<7:30:31,  8.57it/s]\u001b[A\n",
            "Iteration:   0% 85/231657 [00:11<7:43:42,  8.32it/s]\u001b[A\n",
            "Iteration:   0% 86/231657 [00:11<7:42:53,  8.34it/s]\u001b[A\n",
            "Iteration:   0% 87/231657 [00:11<7:37:37,  8.43it/s]\u001b[A\n",
            "Iteration:   0% 88/231657 [00:11<7:28:27,  8.61it/s]\u001b[A\n",
            "Iteration:   0% 89/231657 [00:11<7:26:58,  8.63it/s]\u001b[A\n",
            "Iteration:   0% 90/231657 [00:11<7:15:11,  8.87it/s]\u001b[A\n",
            "Iteration:   0% 91/231657 [00:11<7:19:28,  8.78it/s]\u001b[A\n",
            "Iteration:   0% 92/231657 [00:11<7:23:58,  8.69it/s]\u001b[A\n",
            "Iteration:   0% 93/231657 [00:11<7:44:10,  8.31it/s]\u001b[A\n",
            "Iteration:   0% 94/231657 [00:12<8:18:39,  7.74it/s]\u001b[A\n",
            "Iteration:   0% 95/231657 [00:12<8:01:59,  8.01it/s]\u001b[A\n",
            "Iteration:   0% 96/231657 [00:12<8:04:44,  7.96it/s]\u001b[A\n",
            "Iteration:   0% 97/231657 [00:12<7:54:23,  8.14it/s]\u001b[A\n",
            "Iteration:   0% 98/231657 [00:12<7:43:19,  8.33it/s]\u001b[A\n",
            "Iteration:   0% 99/231657 [00:12<7:54:53,  8.13it/s]\u001b[A\n",
            "Iteration:   0% 100/231657 [00:12<7:50:38,  8.20it/s]\u001b[A\n",
            "Iteration:   0% 101/231657 [00:12<7:50:19,  8.21it/s]\u001b[A\n",
            "Iteration:   0% 102/231657 [00:13<7:47:32,  8.25it/s]\u001b[A\n",
            "Iteration:   0% 103/231657 [00:13<7:41:29,  8.36it/s]\u001b[A\n",
            "Iteration:   0% 104/231657 [00:13<7:41:22,  8.36it/s]\u001b[A\n",
            "Iteration:   0% 105/231657 [00:13<7:23:01,  8.71it/s]\u001b[A\n",
            "Iteration:   0% 106/231657 [00:13<7:30:54,  8.56it/s]\u001b[A\n",
            "Iteration:   0% 107/231657 [00:13<7:24:18,  8.69it/s]\u001b[A\n",
            "Iteration:   0% 108/231657 [00:13<7:09:24,  8.99it/s]\u001b[A\n",
            "Iteration:   0% 109/231657 [00:13<7:21:56,  8.73it/s]\u001b[A\n",
            "Iteration:   0% 110/231657 [00:13<7:30:01,  8.58it/s]\u001b[A\n",
            "Iteration:   0% 111/231657 [00:14<7:29:15,  8.59it/s]\u001b[A\n",
            "Iteration:   0% 112/231657 [00:14<7:36:53,  8.45it/s]\u001b[A\n",
            "Iteration:   0% 113/231657 [00:14<7:36:48,  8.45it/s]\u001b[A\n",
            "Iteration:   0% 114/231657 [00:14<7:20:16,  8.77it/s]\u001b[A\n",
            "Iteration:   0% 115/231657 [00:14<7:16:27,  8.84it/s]\u001b[A\n",
            "Iteration:   0% 116/231657 [00:14<7:18:14,  8.81it/s]\u001b[A\n",
            "Iteration:   0% 117/231657 [00:14<7:22:45,  8.72it/s]\u001b[A\n",
            "Iteration:   0% 118/231657 [00:14<7:13:31,  8.90it/s]\u001b[A\n",
            "Iteration:   0% 119/231657 [00:14<7:18:05,  8.81it/s]\u001b[A\n",
            "Iteration:   0% 120/231657 [00:15<7:10:25,  8.97it/s]\u001b[A\n",
            "Iteration:   0% 121/231657 [00:15<7:20:09,  8.77it/s]\u001b[A\n",
            "Iteration:   0% 122/231657 [00:15<7:18:49,  8.79it/s]\u001b[A\n",
            "Iteration:   0% 123/231657 [00:15<7:14:48,  8.87it/s]\u001b[A\n",
            "Iteration:   0% 124/231657 [00:15<7:07:13,  9.03it/s]\u001b[A\n",
            "Iteration:   0% 125/231657 [00:15<7:11:28,  8.94it/s]\u001b[A\n",
            "Iteration:   0% 126/231657 [00:15<7:02:52,  9.13it/s]\u001b[A\n",
            "Iteration:   0% 127/231657 [00:15<7:12:19,  8.93it/s]\u001b[A\n",
            "Iteration:   0% 128/231657 [00:16<11:09:55,  5.76it/s]\u001b[A\n",
            "Iteration:   0% 129/231657 [00:16<9:54:29,  6.49it/s] \u001b[A\n",
            "Iteration:   0% 130/231657 [00:16<9:01:08,  7.13it/s]\u001b[A\n",
            "Iteration:   0% 131/231657 [00:16<8:28:19,  7.59it/s]\u001b[A\n",
            "Iteration:   0% 132/231657 [00:16<7:56:36,  8.10it/s]\u001b[A\n",
            "Iteration:   0% 133/231657 [00:16<7:46:47,  8.27it/s]\u001b[A\n",
            "Iteration:   0% 134/231657 [00:16<7:25:33,  8.66it/s]\u001b[A\n",
            "Iteration:   0% 135/231657 [00:16<7:42:48,  8.34it/s]\u001b[A\n",
            "Iteration:   0% 136/231657 [00:17<7:52:14,  8.17it/s]\u001b[A\n",
            "Iteration:   0% 137/231657 [00:17<7:46:47,  8.27it/s]\u001b[A\n",
            "Iteration:   0% 138/231657 [00:17<7:39:51,  8.39it/s]\u001b[A\n",
            "Iteration:   0% 139/231657 [00:17<7:25:43,  8.66it/s]\u001b[A\n",
            "Iteration:   0% 140/231657 [00:17<7:23:48,  8.69it/s]\u001b[A\n",
            "Iteration:   0% 141/231657 [00:17<7:11:18,  8.95it/s]\u001b[A\n",
            "Iteration:   0% 142/231657 [00:17<7:27:32,  8.62it/s]\u001b[A\n",
            "Iteration:   0% 143/231657 [00:17<7:27:29,  8.62it/s]\u001b[A\n",
            "Iteration:   0% 144/231657 [00:18<7:45:00,  8.30it/s]\u001b[A\n",
            "Iteration:   0% 145/231657 [00:18<7:47:46,  8.25it/s]\u001b[A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mAiosB2wBm",
        "colab_type": "text"
      },
      "source": [
        "### Compute perplexity of a dataset.\n",
        "This section shows how to compute perplexity of a dataset according to either the pre-trained or your fine-tuned language model. While this is possible to do by calling `run_language_modeling.py` on the command-line as above, we'll instead call the Python functions directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRFwV1Ry3Evk",
        "colab_type": "text"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc2VCFBG3pFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import (\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "def load_model(args):\n",
        "  \"\"\"Creates a model and loads in weights for it.\"\"\"\n",
        "  \n",
        "  config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "  #config_class, model_class, _ = run_language_modeling.MODEL_CLASSES[args.model_type]\n",
        "  #config = config_class.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  model = AutoModelWithLMHead.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "      config=config,\n",
        "      cache_dir=None,\n",
        "  )\n",
        "  model.to(args.device)\n",
        "  return model\n",
        "\n",
        "def set_seed(seed):\n",
        "  \"\"\"Set the random seed.\"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def do_perplexity_eval(args, model, data_file_path):\n",
        "  \"\"\"Computes the perplexity of the text in data_file_path according to the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  args.eval_data_file=data_file_path\n",
        "\n",
        "  #_, _, tokenizer_class = run_language_modeling.MODEL_CLASSES[args.model_type]\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "\n",
        "  result = run_language_modeling.evaluate(args, model, tokenizer, prefix=\"\")\n",
        "  return result\n",
        "  # Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n",
        "# evaluate the pre-trained model without finetuning.\n",
        "!mkdir /content/checkpoint-1500\n",
        "CHECKPOINT_PATH = \"gpt2-medium\"\n",
        "CHECKPOINT_PATH1 = \"ctrl\"\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/shuffle_rewritten_cls.txt\",\n",
        "              ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kClE2Px-j9bb",
        "colab_type": "text"
      },
      "source": [
        "#### Compute it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5o7v2hmhMTO",
        "colab_type": "text"
      },
      "source": [
        "### Generate samples\n",
        "The following code generates text samples that are are continuations of a provided prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcvySe_wrCWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_samples(args, model, prompt_text):\n",
        "  \"\"\"Generating sampling for the provided prompt using the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  #_, _, tokenizer_class = run_language_modeling.MODEL_CLASSES[args.model_type]\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  requires_preprocessing = args.model_type in run_generation.PREPROCESSING_FUNCTIONS.keys()\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=args.length + len(encoded_prompt[0]),\n",
        "      temperature=args.temperature,\n",
        "      top_k=args.k,\n",
        "      top_p=args.p,\n",
        "      no_repeat_ngram_size= args.no_repeat_ngram_size,\n",
        "      repetition_penalty=args.repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=args.num_return_sequences,\n",
        "  )\n",
        "\n",
        "  # Remove the batch dimension when returning multiple sequences\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "\n",
        "  generated_sequences = []\n",
        "\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    # Decode text\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove all text after the stop token\n",
        "    text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "    # Remove the excess text that was used for pre-processing\n",
        "    text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "    # Add the prompt at the beginning of the sequence.\n",
        "    total_sequence = prompt_text + text\n",
        "\n",
        "    generated_sequences.append(total_sequence)\n",
        "\n",
        "  return generated_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3LKo9VVjHw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set this to the checkpoint you want to use for generation, or to \"gpt2-medium\"\n",
        "# to generate with the pre-trained model without finetuning.\n",
        "CHECKPOINT_PATH = '/content/output'\n",
        "\n",
        "# You should try out other prompts as well as no prompt at all.\n",
        "PROMPT = 'I'\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  model_type='gpt2-large',\n",
        "  seed=2,\n",
        "  stop_token='[cls]', # Set this if your dataset has a special word that indicates the end of a text.\n",
        "  temperature=0.75,  # temperature sampling. Set this to temperature=1.0 to not use temperature.\n",
        "  no_repeat_ngram_size= 6,\n",
        "  k=75,  # k for top-k sampling. Set this to k=0 to not use top-k.\n",
        "  p=.95,  # p for nucleus sampling. Set this to p=1.0 to not use nucleus sampling.\n",
        "  repetition_penalty=1,\n",
        "  length=100,  # Number of tokens to generate.\n",
        "  num_return_sequences=8,  # Number of independently computed samples to generate.\n",
        ")\n",
        "args = DictToObj(args)\n",
        "\n",
        "model = load_model(args)\n",
        "sequences = generate_samples(args, model, PROMPT)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0shi42wIa5rM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prompts= [\"JOKE:\",\"JOKE:\",'I',\"What did the blind man say to the otter?\", \n",
        "          \"What did the tomato say to the salad?\", \"Where is my mind?\", \n",
        "          \"She said\", \"He said\", \"I'm tired\", \"Excuse Me\", \"My friend\", \"My friend said\", \"I want to\", \n",
        "          \"I've never\",\"I don't want\", \"My husband\", \"My wife\", \"My brother\",\n",
        "          \"What's going on in there?\", \"What did the dog say to the man?\", \n",
        "          \"What did the dog say to the woman?\"]\n",
        "sequences=[]\n",
        "for PROMPT in prompts:\n",
        "  sequence = generate_samples(args, model, PROMPT)\n",
        "  sequences.append(sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4kuBvfAFaoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import ngrams\n",
        "import pandas as pd\n",
        "\n",
        "def get_data_grams(filename, n=6):\n",
        "    data_grams = set()\n",
        "    with open(filename,'r') as f:\n",
        "      jokes = [line.strip() for line in f if line.strip()!='']\n",
        "      for sequence in jokes:\n",
        "          sixgrams = ngrams(sequence.split(), n)\n",
        "          for gram in sixgrams:\n",
        "              data_grams.add(gram)\n",
        "    return data_grams\n",
        "\n",
        "def not_duplicates(sequences, data_grams, n=6):\n",
        "\n",
        "    sixgrams = ngrams(sequences.split(), n)\n",
        "    result = True\n",
        "    for grams in sixgrams:\n",
        "        if grams in data_grams:\n",
        "            print(True, grams)\n",
        "            result=False\n",
        "            break\n",
        "    return result\n",
        "train_ON ='/content/shuffle_short_cls.txt'\n",
        "data_grams = get_data_grams(train_ON)\n",
        "for idx, sequence in enumerate(sequences):\n",
        "    print('\\n====== GENERATION {} ======'.format(idx))\n",
        "    print(\"Prompt:\",prompts[idx])\n",
        "    for s in sequence:\n",
        "      if not_duplicates(s, data_grams, n=6):\n",
        "        print(s)\n",
        "      else: \n",
        "        print(False,s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9NBd_8wZZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}